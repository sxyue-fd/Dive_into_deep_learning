"""
å¯è§†åŒ–æ¨¡å—
ä¸“é—¨å¤„ç†ResNet18é¡¹ç›®çš„å„ç§å¯è§†åŒ–éœ€æ±‚
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import os
from datetime import datetime
try:
    import cv2
except ImportError:
    print("Warning: OpenCV not found. Some visualization features may not work properly.")
    cv2 = None
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings('ignore')

from utils import denormalize_image


class ResNetVisualizer:
    """ResNetå¯è§†åŒ–å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        # å®‰å…¨åœ°è·å–å¯è§†åŒ–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        self.viz_config = config.get('visualization', {
            'figure_size': (12, 8),
            'dpi': 100,
            'save_format': 'png'
        })
        self.data_config = config.get('data', {})
        self.class_names = [
            'airplane', 'automobile', 'bird', 'cat', 'deer',
            'dog', 'frog', 'horse', 'ship', 'truck'
        ]
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        viz_path = config.get('paths', {}).get('visualizations', 'outputs/visualizations')
        os.makedirs(viz_path, exist_ok=True)
        
    def plot_data_augmentation_examples(self, train_loader, save_path: str = None):
        """ç»˜åˆ¶æ•°æ®å¢å¼ºå‰åå¯¹æ¯” - æ˜¾ç¤ºåŒä¸€å¼ å›¾ç‰‡çš„åŸå§‹ç‰ˆæœ¬ã€æ°´å¹³ç¿»è½¬ç‰ˆæœ¬å’Œéšæœºè£å‰ªç‰ˆæœ¬
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'data_augmentation_examples.png')
        
        # è·å–ä¸€æ‰¹æ•°æ®
        data_iter = iter(train_loader)
        images, labels = next(data_iter)
        
        # é€‰æ‹©å‰8å¼ å›¾ç‰‡
        num_images = min(8, len(images))
        
        fig, axes = plt.subplots(2, num_images, figsize=(2*num_images, 4))
        if num_images == 1:
            axes = axes.reshape(2, 1)
        
        # è·å–å½’ä¸€åŒ–å‚æ•°
        normalize_config = self.data_config.get('transforms', {}).get('normalize', {})
        mean = normalize_config.get('mean', [0.485, 0.456, 0.406])
        std = normalize_config.get('std', [0.229, 0.224, 0.225])
        
        for i in range(num_images):
            # åŸå›¾ï¼ˆå·²å¢å¼ºï¼‰
            img = denormalize_image(images[i], mean, std)
            img = torch.clamp(img, 0, 1)
            img_np = img.permute(1, 2, 0).numpy()
            
            axes[0, i].imshow(img_np)
            axes[0, i].set_title(f'{self.class_names[labels[i]]}')
            axes[0, i].axis('off')
            
            # æ°´å¹³ç¿»è½¬ç‰ˆæœ¬
            flipped_img = torch.flip(img, dims=[2])
            axes[1, i].imshow(flipped_img.permute(1, 2, 0).numpy())
            axes[1, i].set_title('Flipped')
            axes[1, i].axis('off')
        
        plt.suptitle('Data Augmentation Examples', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ•°æ®å¢å¼ºç¤ºä¾‹å·²ä¿å­˜è‡³: {save_path}")

    def plot_training_history(self, history: Dict, save_path: str = None):
        """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿
        
        Args:
            history: è®­ç»ƒå†å²è®°å½•ï¼ŒåŒ…å« train_loss, val_loss, train_acc, val_acc
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(viz_path, f'training_history_{timestamp}.png')
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(history['train_loss']) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)
        ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epochs')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, [acc * 100 for acc in history['train_acc']], 'b-', label='Training Accuracy', linewidth=2)
        ax2.plot(epochs, [acc * 100 for acc in history['val_acc']], 'r-', label='Validation Accuracy', linewidth=2)
        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epochs')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡å˜åŒ–ï¼ˆå¦‚æœæœ‰è®°å½•ï¼‰
        if 'learning_rate' in history:
            ax3.plot(epochs, history['learning_rate'], 'g-', linewidth=2)
            ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Epochs')
            ax3.set_ylabel('Learning Rate')
            ax3.set_yscale('log')
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'Learning Rate History\nNot Available', ha='center', va='center',
                    transform=ax3.transAxes, fontsize=12)
            ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        
        # è®­ç»ƒéªŒè¯å·®è·
        acc_gap = [train - val for train, val in zip(history['train_acc'], history['val_acc'])]
        ax4.plot(epochs, acc_gap, 'purple', linewidth=2)
        ax4.set_title('Training-Validation Accuracy Gap', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Epochs')
        ax4.set_ylabel('Accuracy Gap (%)')
        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š è®­ç»ƒå†å²æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")

    def plot_confusion_matrix(self, model, test_loader, device, save_path: str = None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'confusion_matrix.png')
        
        model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(all_labels, all_preds)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=self.class_names, yticklabels=self.class_names)
        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")

    def plot_feature_maps(self, model, input_tensor, layer_name: str = None, save_path: str = None):
        """å¯è§†åŒ–ç‰¹å¾å›¾
        
        Args:
            model: æ¨¡å‹
            input_tensor: è¾“å…¥å¼ é‡
            layer_name: å±‚åç§°
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'feature_maps.png')
        
        model.eval()
        activation = {}
        
        def get_activation(name):
            def hook(model, input, output):
                activation[name] = output.detach()
            return hook
        
        # æ³¨å†Œé’©å­åˆ°ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        if hasattr(model, 'conv1'):
            model.conv1.register_forward_hook(get_activation('conv1'))
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = model(input_tensor.unsqueeze(0))
        
        # å¯è§†åŒ–ç‰¹å¾å›¾
        if 'conv1' in activation:
            features = activation['conv1'].squeeze(0)  # ç§»é™¤batchç»´åº¦
            num_features = min(16, features.size(0))  # æœ€å¤šæ˜¾ç¤º16ä¸ªç‰¹å¾å›¾
            
            fig, axes = plt.subplots(4, 4, figsize=(12, 12))
            for i in range(num_features):
                row, col = i // 4, i % 4
                axes[row, col].imshow(features[i].cpu().numpy(), cmap='viridis')
                axes[row, col].set_title(f'Feature {i+1}')
                axes[row, col].axis('off')
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(num_features, 16):
                row, col = i // 4, i % 4
                axes[row, col].axis('off')
            
            plt.suptitle('Feature Maps from First Convolutional Layer', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š ç‰¹å¾å›¾å·²ä¿å­˜è‡³: {save_path}")

    def plot_model_architecture(self, model, save_path: str = None):
        """å¯è§†åŒ–æ¨¡å‹æ¶æ„
        
        Args:
            model: æ¨¡å‹
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'model_architecture.png')
        
        # è¿™é‡Œå¯ä»¥å®ç°æ¨¡å‹æ¶æ„å›¾çš„ç»˜åˆ¶
        # ç”±äºå¤æ‚æ€§ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç®€å•çš„æ–‡æœ¬è¡¨ç¤º
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.text(0.5, 0.5, str(model), ha='center', va='center', 
                fontsize=8, transform=ax.transAxes, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        ax.set_title('ResNet18 Architecture', fontsize=16, fontweight='bold')
        ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ¨¡å‹æ¶æ„å›¾å·²ä¿å­˜è‡³: {save_path}")    
    def visualize_feature_maps(self, model, input_images, layer_names: List[str], save_path: str = None):
        """å¯è§†åŒ–æŒ‡å®šå±‚çš„ç‰¹å¾å›¾ï¼ŒåŒ…æ‹¬åŸå§‹å›¾ç‰‡å’Œåˆå§‹å·ç§¯å±‚
        
        Args:
            model: æ¨¡å‹
            input_images: è¾“å…¥å›¾åƒå¼ é‡ (batch_size, C, H, W)
            layer_names: è¦å¯è§†åŒ–çš„å±‚åç§°åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'feature_maps.png')
        
        model.eval()
        activations = {}
        
        def get_activation(name):
            def hook(model, input, output):
                activations[name] = output.detach()
            return hook
        
        # æ³¨å†Œé’©å­åˆ°æŒ‡å®šå±‚å’Œåˆå§‹å·ç§¯å±‚
        handles = []
        
        # æ·»åŠ åˆå§‹å·ç§¯å±‚
        if hasattr(model, 'conv1'):
            handle = model.conv1.register_forward_hook(get_activation('conv1'))
            handles.append(handle)
        
        # æ·»åŠ æŒ‡å®šçš„å±‚
        for layer_name in layer_names:
            if hasattr(model, layer_name):
                layer = getattr(model, layer_name)
                if hasattr(layer, 'register_forward_hook'):
                    handle = layer.register_forward_hook(get_activation(layer_name))
                    handles.append(handle)
        
        # å‰å‘ä¼ æ’­
        device = next(model.parameters()).device
        if len(input_images) > 1:
            input_images = input_images[:1]  # åªä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡
        
        input_tensor = input_images.to(device)
        
        with torch.no_grad():
            _ = model(input_tensor)
          # å‡†å¤‡å¯è§†åŒ– - æ°´å¹³å¸ƒå±€ï¼š4è¡Œ x (2 + len(layer_names))åˆ—
        # ç¬¬1åˆ—ï¼šåŸå§‹å›¾ç‰‡ï¼Œç¬¬2åˆ—ï¼šconv1ï¼Œåç»­åˆ—ï¼šå„å±‚ç‰¹å¾å›¾
        total_cols = 2 + len(layer_names)
        fig, axes = plt.subplots(4, total_cols, figsize=(4*total_cols, 12))
        
        # è·å–å½’ä¸€åŒ–å‚æ•°ç”¨äºæ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        normalize_config = self.data_config.get('transforms', {}).get('normalize', {})
        mean = normalize_config.get('mean', [0.485, 0.456, 0.406])
        std = normalize_config.get('std', [0.229, 0.224, 0.225])
        
        # ç¬¬ä¸€åˆ—ï¼šæ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        original_img = denormalize_image(input_tensor[0].cpu(), mean, std)
        original_img = torch.clamp(original_img, 0, 1)
        
        # ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œï¼šæ˜¾ç¤ºRGBåŸå›¾
        axes[0, 0].imshow(original_img.permute(1, 2, 0).numpy())
        axes[0, 0].set_title('Original\nImage', fontsize=10, fontweight='bold')
        axes[0, 0].axis('off')
        
        # ç¬¬ä¸€åˆ—å…¶ä½™è¡Œï¼šæ˜¾ç¤ºRGBä¸‰ä¸ªé€šé“
        channel_names = ['Red Channel', 'Green Channel', 'Blue Channel']
        for i in range(3):
            axes[i+1, 0].imshow(original_img[i].numpy(), cmap='gray')
            axes[i+1, 0].set_title(channel_names[i], fontsize=10)
            axes[i+1, 0].axis('off')
        
        # ç¬¬äºŒåˆ—ï¼šæ˜¾ç¤ºconv1ç‰¹å¾å›¾ï¼ˆå‰4ä¸ªé€šé“ï¼‰
        if 'conv1' in activations:
            conv1_features = activations['conv1'][0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
            for i in range(4):
                if i < conv1_features.size(0):
                    feature_map = conv1_features[i].cpu().numpy()
                    axes[i, 1].imshow(feature_map, cmap='viridis')
                    axes[i, 1].set_title(f'Conv1\nCh{i+1}', fontsize=10)
                else:
                    axes[i, 1].axis('off')
                axes[i, 1].set_xticks([])
                axes[i, 1].set_yticks([])
        else:
            for i in range(4):
                axes[i, 1].axis('off')
                axes[i, 1].text(0.5, 0.5, 'Conv1\nNot Available', ha='center', va='center', 
                               transform=axes[i, 1].transAxes)
        
        # åç»­åˆ—ï¼šæ˜¾ç¤ºå…¶ä»–å±‚çš„ç‰¹å¾å›¾ï¼ˆæ¯å±‚æ˜¾ç¤ºå‰4ä¸ªé€šé“ï¼‰
        col_idx = 2
        for layer_name in layer_names:
            if layer_name in activations:
                features = activations[layer_name][0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾
                for i in range(4):
                    if i < features.size(0):
                        feature_map = features[i].cpu().numpy()
                        axes[i, col_idx].imshow(feature_map, cmap='viridis')
                        axes[i, col_idx].set_title(f'{layer_name}\nCh{i+1}', fontsize=10)
                    else:
                        axes[i, col_idx].axis('off')
                    axes[i, col_idx].set_xticks([])
                    axes[i, col_idx].set_yticks([])
            else:
                for i in range(4):
                    axes[i, col_idx].axis('off')
                    axes[i, col_idx].text(0.5, 0.5, f'{layer_name}\nNot Available', ha='center', va='center',
                                         transform=axes[i, col_idx].transAxes)
            col_idx += 1
        
        plt.suptitle('Feature Maps Visualization (Original â†’ Conv1 â†’ Layer1 â†’ Layer2 â†’ Layer3 â†’ Layer4)', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ç‰¹å¾å›¾å·²ä¿å­˜è‡³: {save_path}")
        
        # ç§»é™¤é’©å­
        for handle in handles:
            handle.remove()

    def visualize_activation_heatmaps(self, model, input_images, save_path: str = None):
        """å¯è§†åŒ–æ¿€æ´»çƒ­åŠ›å›¾ï¼ˆä½¿ç”¨Grad-CAMæŠ€æœ¯ï¼‰
        
        Args:
            model: æ¨¡å‹
            input_images: è¾“å…¥å›¾åƒå¼ é‡
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'activation_heatmaps.png')
        
        model.eval()
        device = next(model.parameters()).device
        
        # é€‰æ‹©å‰4å¼ å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–
        num_images = min(4, len(input_images))
        images = input_images[:num_images].to(device)
        
        # è·å–å½’ä¸€åŒ–å‚æ•°ç”¨äºåæ ‡å‡†åŒ–æ˜¾ç¤º
        normalize_config = self.data_config.get('transforms', {}).get('normalize', {})
        mean = normalize_config.get('mean', [0.485, 0.456, 0.406])
        std = normalize_config.get('std', [0.229, 0.224, 0.225])
        
        fig, axes = plt.subplots(2, num_images, figsize=(3*num_images, 6))
        if num_images == 1:
            axes = axes.reshape(2, 1)
        
        for i in range(num_images):
            # åŸå§‹å›¾åƒ
            img = denormalize_image(images[i].cpu(), mean, std)
            img = torch.clamp(img, 0, 1)
            axes[0, i].imshow(img.permute(1, 2, 0).numpy())
            axes[0, i].set_title(f'Original Image {i+1}')
            axes[0, i].axis('off')
            
            # ç®€å•çš„æ¿€æ´»å¯è§†åŒ–ï¼ˆä½¿ç”¨æœ€åä¸€å±‚ç‰¹å¾çš„å¹³å‡å€¼ï¼‰
            with torch.no_grad():
                # è·å–æ¨¡å‹çš„æœ€åå·ç§¯å±‚è¾“å‡º
                if hasattr(model, 'layer4'):
                    x = images[i:i+1]                    # å‰å‘ä¼ æ’­åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼ˆé€‚é…æˆ‘ä»¬çš„ResNet18ç»“æ„ï¼‰
                    x = model.conv1(x)
                    x = model.bn1(x)
                    x = model.relu(x)
                    x = model.layer1(x)
                    x = model.layer2(x)
                    x = model.layer3(x)
                    x = model.layer4(x)# å¯¹ç‰¹å¾å›¾æ±‚å¹³å‡å¾—åˆ°çƒ­åŠ›å›¾
                    heatmap = torch.mean(x[0], dim=0).cpu().numpy()
                    # è°ƒæ•´åˆ°åŸå›¾å¤§å°
                    if cv2 is not None:
                        heatmap = cv2.resize(heatmap, (32, 32))
                    else:
                        # å¦‚æœæ²¡æœ‰cv2ï¼Œä½¿ç”¨PyTorchçš„æ’å€¼
                        heatmap_tensor = torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0)
                        heatmap_tensor = F.interpolate(heatmap_tensor, size=(32, 32), mode='bilinear', align_corners=False)
                        heatmap = heatmap_tensor.squeeze().numpy()
                    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())
                else:
                    # å¦‚æœæ²¡æœ‰layer4ï¼Œåˆ›å»ºä¸€ä¸ªéšæœºçƒ­åŠ›å›¾ä½œä¸ºå ä½ç¬¦
                    heatmap = np.random.rand(32, 32)
            
            # æ˜¾ç¤ºçƒ­åŠ›å›¾
            im = axes[1, i].imshow(heatmap, cmap='jet', alpha=0.7)
            axes[1, i].imshow(img.permute(1, 2, 0).numpy(), alpha=0.3)
            axes[1, i].set_title(f'Activation Heatmap {i+1}')
            axes[1, i].axis('off')
        
        plt.suptitle('Activation Heatmaps Visualization', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ¿€æ´»çƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {save_path}")

    def plot_class_distribution(self, data_loader, save_path: str = None):
        """ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒå›¾
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'class_distribution.png')
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        class_counts = np.zeros(len(self.class_names))
        
        for _, labels in data_loader:
            for label in labels:
                class_counts[label.item()] += 1
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æŸ±çŠ¶å›¾
        bars = ax1.bar(range(len(self.class_names)), class_counts, 
                      color=plt.cm.tab10(np.linspace(0, 1, len(self.class_names))))
        ax1.set_xlabel('Classes')
        ax1.set_ylabel('Number of Samples')
        ax1.set_title('Class Distribution - Bar Chart')
        ax1.set_xticks(range(len(self.class_names)))
        ax1.set_xticklabels(self.class_names, rotation=45, ha='right')
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + max(class_counts)*0.01,
                    f'{int(height)}', ha='center', va='bottom', fontsize=9)
        
        # é¥¼å›¾
        ax2.pie(class_counts, labels=self.class_names, autopct='%1.1f%%', startangle=90,
               colors=plt.cm.tab10(np.linspace(0, 1, len(self.class_names))))
        ax2.set_title('Class Distribution - Pie Chart')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {save_path}")

    def denormalize_image(tensor: torch.Tensor, mean: List[float], std: List[float]) -> torch.Tensor:
        """åæ ‡å‡†åŒ–å›¾åƒå¼ é‡ç”¨äºå¯è§†åŒ–
        
        Args:
            tensor: æ ‡å‡†åŒ–åçš„å›¾åƒå¼ é‡ (C, H, W)
            mean: æ ‡å‡†åŒ–å‡å€¼
            std: æ ‡å‡†åŒ–æ ‡å‡†å·®
            
        Returns:
            åæ ‡å‡†åŒ–åçš„å›¾åƒå¼ é‡
        """
        tensor = tensor.clone()
        for t, m, s in zip(tensor, mean, std):
            t.mul_(s).add_(m)
        return tensor
    
    def visualize_conv_kernels(self, model, save_path: str = None):
        """å¯è§†åŒ–ç¬¬ä¸€å±‚å·ç§¯æ ¸æƒé‡
        
        Args:
            model: æ¨¡å‹
            save_path: ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            viz_path = self.config.get('paths', {}).get('visualizations', 'outputs/visualizations')
            save_path = os.path.join(viz_path, 'conv_kernels.png')
          # è·å–ç¬¬ä¸€å±‚å·ç§¯å±‚çš„æƒé‡
        if hasattr(model, 'conv1'):
            conv1_weights = model.conv1.weight.data.cpu()  # shape: (out_channels, in_channels, h, w)
            num_kernels = min(4, conv1_weights.size(0))  # åªæ˜¾ç¤ºå‰4ä¸ªå·ç§¯æ ¸
            in_channels = conv1_weights.size(1)  # è¾“å…¥é€šé“æ•° (é€šå¸¸æ˜¯3ï¼Œå¯¹åº”RGB)
            
            # åˆ›å»ºå¸ƒå±€ï¼šæ¯ä¸ªè¾“å…¥é€šé“ä¸€è¡Œï¼Œ4ä¸ªå·ç§¯æ ¸ä¸€è¡Œæ’åˆ—
            fig, axes = plt.subplots(in_channels, 4, figsize=(10, 2.5 * in_channels))
            if in_channels == 1:
                axes = [axes]
            
            channel_names = ['Red', 'Green', 'Blue'] if in_channels == 3 else [f'Channel {i}' for i in range(in_channels)]
            
            for ch in range(in_channels):
                for i in range(4):
                    if i < num_kernels:
                        # è·å–å½“å‰å·ç§¯æ ¸çš„æƒé‡
                        kernel = conv1_weights[i, ch, :, :].numpy()
                        
                        # æ ‡å‡†åŒ–åˆ° [0, 1] èŒƒå›´ä¾¿äºæ˜¾ç¤º
                        kernel_norm = (kernel - kernel.min()) / (kernel.max() - kernel.min() + 1e-8)
                        
                        # æ˜¾ç¤ºå·ç§¯æ ¸
                        axes[ch][i].imshow(kernel_norm, cmap='viridis')
                        axes[ch][i].set_title(f'Kernel {i+1}', fontsize=10)
                        axes[ch][i].axis('off')
                    else:
                        # éšè—å¤šä½™çš„å­å›¾
                        axes[ch][i].axis('off')
                
                # ä¸ºæ¯è¡Œæ·»åŠ é€šé“æ ‡ç­¾
                axes[ch][0].text(-0.15, 0.5, f'{channel_names[ch]}\nChannel', 
                               rotation=90, ha='center', va='center', 
                               transform=axes[ch][0].transAxes, fontsize=11, fontweight='bold')
            
            plt.suptitle(f'First 4 Convolutional Kernels Visualization\n(Conv1 Layer: {num_kernels} kernels, {in_channels} input channels)', 
                        fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š å·ç§¯æ ¸å¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")
        else:
            print("âŒ æ¨¡å‹ä¸­æœªæ‰¾åˆ°conv1å±‚ï¼Œæ— æ³•è¿›è¡Œå·ç§¯æ ¸å¯è§†åŒ–")
