"""
ResNet18 è®­ç»ƒå™¨æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import time
import logging
from typing import Dict, Tuple, List, Optional
import numpy as np
from datetime import datetime
import os
from pathlib import Path  # æ·»åŠ å¯¼å…¥

from model import ResNet18
from utils import AverageMeter, EarlyStopping, save_checkpoint, calculate_accuracy
from performance_config import get_early_stopping_config


class ResNetTrainer:
    """ResNet18è®­ç»ƒå™¨"""
    
    def __init__(self, model: ResNet18, config: Dict):
        self.model = model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() and config['device']['use_cuda'] else 'cpu')
        self.logger = logging.getLogger(__name__)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = config['device']['mixed_precision'] and self.device.type == 'cuda'
        self.scaler = GradScaler() if self.use_amp else None
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self._setup_optimizer()
        self._setup_scheduler()
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()        # æ—©åœæœºåˆ¶ - ä½¿ç”¨å…¼å®¹æ€§å‡½æ•°
        early_stopping_config = get_early_stopping_config(config)
        
        self.early_stopping = EarlyStopping(
            patience=early_stopping_config['patience'],
            min_delta=early_stopping_config['min_delta'],
            mode=early_stopping_config['mode']
        )
        
        # æ¸…ç†æ—§çš„æ¨¡å‹æ–‡ä»¶
        self._clean_previous_models()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'val_top5_acc': [],
            'lr': []
        }
          # æœ€ä½³æŒ‡æ ‡
        self.best_val_acc = 0.0
        self.best_epoch = 0
        
        # è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        self.training_duration = 0.0
        self.stop_reason = "Training not started"
        
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        train_config = self.config['training']
        # è·å–ä¼˜åŒ–å™¨ç‰¹å®šå‚æ•°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸ºç©ºå­—å…¸ï¼Œå¹¶å¤åˆ¶ä»¥è¿›è¡Œæœ¬åœ°ä¿®æ”¹
        optimizer_params = train_config.get('optimizer_params', {}).copy() 
        
        # ç¡®ä¿é€šç”¨å‚æ•°æ˜¯æµ®ç‚¹æ•°
        learning_rate = float(train_config['learning_rate'])
        weight_decay = float(train_config['weight_decay'])

        if train_config['optimizer'].lower() == 'adam':
            # ç¡®ä¿Adamç‰¹å®šçš„å‚æ•°ç±»å‹æ­£ç¡®
            if 'eps' in optimizer_params:
                optimizer_params['eps'] = float(optimizer_params['eps'])
            if 'betas' in optimizer_params and isinstance(optimizer_params['betas'], list):
                optimizer_params['betas'] = [float(b) for b in optimizer_params['betas']]
            
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay,
                **optimizer_params # ä¼ é€’å¤„ç†è¿‡çš„å‚æ•°
            )
        elif train_config['optimizer'].lower() == 'sgd':
            # ç¡®ä¿SGDçš„momentumæ˜¯æµ®ç‚¹æ•°ï¼Œå¹¶ä»train_configè·å–
            momentum = float(train_config.get('momentum', 0.9)) # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤å€¼
            
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=momentum,
                weight_decay=weight_decay,
                nesterov=optimizer_params.get('nesterov', False) # nesterové€šå¸¸åœ¨optimizer_paramsä¸­
            )
        else:            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {train_config['optimizer']}")
            
    def _setup_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        train_config = self.config['training']
        scheduler_name = train_config.get('scheduler', '').lower()
        
        if not scheduler_name or scheduler_name == 'none':
            self.scheduler = None
            return
            
        scheduler_params = train_config.get('scheduler_params', {}).copy()

        if scheduler_name == 'cosine':
            # è¿‡æ»¤å‡ºCosineAnnealingLRçš„æœ‰æ•ˆå‚æ•°
            cosine_params = {}
            if 'T_max' in scheduler_params:
                cosine_params['T_max'] = int(scheduler_params['T_max'])
            if 'eta_min' in scheduler_params:
                cosine_params['eta_min'] = float(scheduler_params['eta_min'])
            
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                **cosine_params
            )
        elif scheduler_name == 'step':
            # è¿‡æ»¤å‡ºStepLRçš„æœ‰æ•ˆå‚æ•°
            step_params = {}
            if 'step_size' in scheduler_params:
                step_params['step_size'] = int(scheduler_params['step_size'])
            if 'gamma' in scheduler_params:
                step_params['gamma'] = float(scheduler_params['gamma'])

            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                **step_params
            )        
        else:
            self.logger.warning(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_name}ï¼Œå°†ä¸ä½¿ç”¨è°ƒåº¦å™¨")
            self.scheduler = None
    
    def _clean_previous_models(self):
        """æ¸…ç†æ‰€æœ‰ä¹‹å‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œä¸ºæ–°è®­ç»ƒåšå‡†å¤‡"""
        models_dir = Path(self.config['paths']['models'])
        
        if models_dir.exists():
            # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
            model_files = list(models_dir.glob('*.pth')) + list(models_dir.glob('*.pt'))
            
            if model_files:
                self.logger.info(f"æ¸…ç† {len(model_files)} ä¸ªæ—§æ¨¡å‹æ–‡ä»¶...")
                for model_file in model_files:
                    try:
                        model_file.unlink()
                        self.logger.debug(f"å·²åˆ é™¤: {model_file.name}")
                    except Exception as e:
                        self.logger.warning(f"åˆ é™¤ {model_file.name} å¤±è´¥: {e}")
                
                self.logger.info("æ¨¡å‹æ–‡ä»¶æ¸…ç†å®Œæˆ")
            else:
                self.logger.info("æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„æ—§æ¨¡å‹æ–‡ä»¶")
        else:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            models_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"åˆ›å»ºæ¨¡å‹ç›®å½•: {models_dir}")
            
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        """
        self.model.train()
        
        loss_meter = AverageMeter()
        acc_meter = AverageMeter()
        
        start_time = time.time()
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            acc = calculate_accuracy(output, target, topk=(1,))[0]
            
            # æ›´æ–°æŒ‡æ ‡
            loss_meter.update(loss.item(), data.size(0))
            acc_meter.update(acc.item(), data.size(0))
            
            # è®°å½•æ—¥å¿—
            if batch_idx % self.config['logging']['log_frequency'] == 0:
                self.logger.info(
                    f'è®­ç»ƒ Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                    f'({100. * batch_idx / len(train_loader):.0f}%)]\t'
                    f'Loss: {loss.item():.6f}\tAcc: {acc.item():.2f}%'
                )
        
        epoch_time = time.time() - start_time
        self.logger.info(f'Epoch {epoch} è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {epoch_time:.2f}s')
        
        return loss_meter.avg, acc_meter.avg
    
    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float, float]:
        """éªŒè¯ä¸€ä¸ªepoch
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡æŸå¤±ã€Top-1å‡†ç¡®ç‡å’ŒTop-5å‡†ç¡®ç‡
        """
        self.model.eval()
        
        loss_meter = AverageMeter()
        acc1_meter = AverageMeter()
        acc5_meter = AverageMeter()
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                # è®¡ç®—Top-1å’ŒTop-5å‡†ç¡®ç‡
                acc1, acc5 = calculate_accuracy(output, target, topk=(1, 5))
                
                # æ›´æ–°æŒ‡æ ‡
                loss_meter.update(loss.item(), data.size(0))
                acc1_meter.update(acc1.item(), data.size(0))
                acc5_meter.update(acc5.item(), data.size(0))
        
        return loss_meter.avg, acc1_meter.avg, acc5_meter.avg
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        """
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"è®¾å¤‡: {self.device}")
        self.logger.info(f"æ··åˆç²¾åº¦: {self.use_amp}")
        
        start_time = time.time()
        
        for epoch in range(1, self.config['training']['epochs'] + 1):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_loss, val_acc, val_top5_acc = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
            
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.train_history['epoch'].append(epoch)
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['val_top5_acc'].append(val_top5_acc)
            self.train_history['lr'].append(current_lr)
            
            epoch_time = time.time() - epoch_start
            
            # è®°å½•epochç»“æœ
            self.logger.info(
                f'Epoch {epoch}/{self.config["training"]["epochs"]} - '
                f'è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% - '
                f'éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%, '
                f'Top-5å‡†ç¡®ç‡: {val_top5_acc:.2f}% - '
                f'å­¦ä¹ ç‡: {current_lr:.6f} - è€—æ—¶: {epoch_time:.2f}s'
            )            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self.best_epoch = epoch
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                        'best_val_acc': self.best_val_acc,
                        'train_history': self.train_history,
                        'config': self.config
                    },
                    is_best=True,
                    checkpoint_dir=self.config['paths']['models'],
                    filename='best.pth'
                )
            
            # å‘¨æœŸæ€§ä¿å­˜æ£€æŸ¥ç‚¹
            save_frequency = self.config.get('checkpoint', {}).get('save_frequency', 5)
            if epoch % save_frequency == 0:
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                        'best_val_acc': self.best_val_acc,
                        'train_history': self.train_history,
                        'config': self.config
                    },
                    is_best=False,
                    checkpoint_dir=self.config['paths']['models'],
                    filename=f'checkpoint_epoch_{epoch}.pth'
                )
            
            # æ—©åœæ£€æŸ¥
            self.early_stopping(val_loss)
            
            # ç»¼åˆåœæ­¢åˆ¤æ–­
            should_stop, stop_reason = self.should_stop_training(epoch)
            if should_stop:
                self.logger.info(f"è®­ç»ƒåœæ­¢ - {stop_reason}")
                self.stop_reason = stop_reason  # è®°å½•åœæ­¢åŸå› 
                
                # å¦‚æœæ˜¯å› ä¸ºè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡è€Œåœæ­¢ï¼Œè®°å½•ç‰¹æ®Šä¿¡æ¯
                if "Target accuracy" in stop_reason:
                    self.logger.info("ğŸ‰ æ­å–œï¼æ¨¡å‹å·²è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ï¼")
                
                break
        
        total_time = time.time() - start_time
        self.training_duration = total_time  # è®°å½•è®­ç»ƒæ—¶é•¿
        
        # å¦‚æœæ­£å¸¸å®Œæˆè®­ç»ƒï¼ˆæ²¡æœ‰æå‰åœæ­¢ï¼‰
        if not hasattr(self, 'stop_reason') or self.stop_reason == "Training not started":
            self.stop_reason = "Normal completion - all epochs finished"
        
        self.logger.info(f"è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}s")
        self.logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}% (Epoch {self.best_epoch})")
        
        return self.train_history

    def should_stop_training(self, epoch: int) -> Tuple[bool, str]:
        """ç»¼åˆåˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        
        Args:
            epoch: å½“å‰epochæ•°
            
        Returns:
            (should_stop, reason): æ˜¯å¦åœæ­¢å’Œåœæ­¢åŸå› 
        """
        # 1. è¾¾åˆ°æœ€å¤§epochæ•°
        if epoch >= self.config['training']['epochs']:
            return True, "Maximum epochs reached"
          # 2. éªŒè¯å‡†ç¡®ç‡è¾¾åˆ°ç›®æ ‡ï¼ˆä¸»è¦åœæ­¢æ¡ä»¶ï¼‰
        target_acc = float(self.config['training'].get('target_accuracy', 0.95))
        if self.train_history['val_acc'] and len(self.train_history['val_acc']) > 0:
            current_val_acc = self.train_history['val_acc'][-1] / 100.0  # è½¬æ¢ä¸ºå°æ•°
            if current_val_acc >= target_acc:
                return True, f"ğŸ¯ Target accuracy {target_acc:.1%} achieved (current: {current_val_acc:.1%})"
        
        # 3. æ—©åœæ£€æŸ¥ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆçš„ä¸»è¦æœºåˆ¶ï¼‰
        if self.early_stopping.early_stop:
            return True, "â¹ï¸ Early stopping triggered - validation loss not improving (preventing overfitting)"
          # 4. å­¦ä¹ ç‡è¿‡å°æ£€æŸ¥
        current_lr = self.optimizer.param_groups[0]['lr']
        min_lr = float(self.config['training'].get('min_learning_rate', 1e-8))
        if current_lr < min_lr:
            return True, f"ğŸ“‰ Learning rate too small: {current_lr:.2e} < {min_lr:.2e}"
          # è¾…åŠ©æ£€æµ‹ï¼ˆç”¨äºæä¾›é¢å¤–ä¿¡æ¯ï¼Œä½†ä¸ç›´æ¥åœæ­¢è®­ç»ƒï¼‰
        warnings = []
        
        # æŸå¤±æ”¶æ•›æ£€æµ‹
        convergence_config = self.config['training'].get('convergence', {})
        convergence_patience = int(convergence_config.get('patience', 15))  # å¢åŠ å®¹å¿åº¦
        convergence_threshold = float(convergence_config.get('threshold', 1e-5))  # æ›´ä¸¥æ ¼çš„é˜ˆå€¼
        
        if len(self.train_history['val_loss']) >= convergence_patience:
            recent_losses = self.train_history['val_loss'][-convergence_patience:]
            loss_std = np.std(recent_losses)
            if loss_std < convergence_threshold:
                warnings.append(f"Validation loss converged (std: {loss_std:.2e})")
        
        # è¿‡æ‹Ÿåˆç¨‹åº¦æ£€æµ‹ï¼ˆä»…è­¦å‘Šï¼Œä¸åœæ­¢ï¼‰
        overfitting_config = self.config['training'].get('overfitting', {})
        overfitting_patience = int(overfitting_config.get('patience', 5))
        gap_threshold = float(overfitting_config.get('train_val_gap_threshold', 0.15))  # æ›´å®½æ¾çš„é˜ˆå€¼
        
        if (len(self.train_history['train_acc']) >= overfitting_patience and 
            len(self.train_history['val_acc']) >= overfitting_patience):
            
            recent_train_acc = self.train_history['train_acc'][-overfitting_patience:]
            recent_val_acc = self.train_history['val_acc'][-overfitting_patience:]
            
            # è®¡ç®—æœ€è¿‘å‡ ä¸ªepochçš„å¹³å‡å‡†ç¡®ç‡å·®è·
            avg_gap = np.mean([t - v for t, v in zip(recent_train_acc, recent_val_acc)]) / 100.0
            
            if avg_gap > gap_threshold:
                warnings.append(f"High train-val gap detected: {avg_gap:.1%}")
          # è®°å½•è­¦å‘Šä¿¡æ¯ï¼ˆä½†ä¸åœæ­¢è®­ç»ƒï¼‰
        if warnings:
            self.logger.warning(f"Training monitoring alerts: {'; '.join(warnings)}")
        
        return False, ""
    
    def check_target_accuracy_reached(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡"""
        target_acc = float(self.config['training'].get('target_accuracy', 0.95))
        if self.train_history['val_acc'] and len(self.train_history['val_acc']) > 0:
            current_val_acc = self.train_history['val_acc'][-1] / 100.0  # è½¬æ¢ä¸ºå°æ•°
            return current_val_acc >= target_acc
        return False


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import yaml
    from pathlib import Path  # æ·»åŠ å¯¼å…¥
    from data import create_data_loaders
    from model import create_resnet18
    from utils import setup_logging, set_random_seed
    
    # æ„å»ºæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„
    current_file_dir = Path(__file__).parent
    config_path = current_file_dir.parent / 'configs' / 'config.yaml'

    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®æ—¥å¿—å’Œéšæœºç§å­
    # setup_logging(config) # æ—¥å¿—è®¾ç½®å¯èƒ½ä¹Ÿéœ€è¦è°ƒæ•´è·¯å¾„ï¼Œæš‚æ—¶æ³¨é‡Š
    set_random_seed(config['data']['random_seed']) # ä»dataä¸‹è·å–random_seed
    
    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
    model = create_resnet18(num_classes=config['data'].get('num_classes', 10)) # ç¡®ä¿num_classeså­˜åœ¨
    train_loader, val_loader, test_loader = create_data_loaders(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ResNetTrainer(model, config)
    
    print("è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸï¼")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
